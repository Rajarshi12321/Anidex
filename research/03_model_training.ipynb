{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.chdir(\"../\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dataclasses import dataclass\n",
                "from pathlib import Path\n",
                "\n",
                "\n",
                "@dataclass(frozen=True)\n",
                "class TrainingConfig:\n",
                "    root_dir: Path\n",
                "    trained_model_path: Path\n",
                "    updated_base_model_path: Path\n",
                "    training_data: Path\n",
                "    params_epochs: int\n",
                "    params_batch_size: int\n",
                "    params_is_augmentation: bool\n",
                "    params_image_size: list\n",
                "    mlflow_uri: str"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from anidex.constants import *\n",
                "from anidex.utils.common import read_yaml, create_directories\n",
                "import tensorflow as tf"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ConfigurationManager:\n",
                "    def __init__(\n",
                "        self,\n",
                "        config_filepath = CONFIG_FILE_PATH,\n",
                "        params_filepath = PARAMS_FILE_PATH):\n",
                "\n",
                "        self.config = read_yaml(config_filepath)\n",
                "        self.params = read_yaml(params_filepath)\n",
                "\n",
                "        create_directories([self.config.artifacts_root])\n",
                "\n",
                "\n",
                "    \n",
                "    # def get_model_training_config(self) -> TrainingConfig:\n",
                "    #     config = self.config.training\n",
                "        \n",
                "    #     create_directories([config.root_dir])\n",
                "\n",
                "    #     model_training_config = TrainingConfig(\n",
                "    #         root_dir=config.root_dir,\n",
                "    #         training_data=config.training_data,\n",
                "    #         trained_model_file_path=config.trained_model_file_path,\n",
                "    #         trained_model_file_path_rent=config.trained_model_file_path_rent,\n",
                "    #         mlflow_uri=config.mlflow_uri,\n",
                "    #         model_params = self.params\n",
                " \n",
                "    #     )\n",
                "\n",
                "    #     return model_training_config\n",
                "    \n",
                "    def get_training_config(self) -> TrainingConfig:\n",
                "        config = self.config\n",
                "        training = self.config.training\n",
                "        prepare_base_model = self.config.prepare_base_model\n",
                "        params = self.params\n",
                "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"animals\")\n",
                "        create_directories([\n",
                "            Path(training.root_dir)\n",
                "        ])\n",
                "\n",
                "        training_config = TrainingConfig(\n",
                "            root_dir=Path(training.root_dir),\n",
                "            trained_model_path=Path(training.trained_model_path),\n",
                "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
                "            training_data=Path(training_data),\n",
                "            params_epochs=params.EPOCHS,\n",
                "            params_batch_size=params.BATCH_SIZE,\n",
                "            params_is_augmentation=params.AUGMENTATION,\n",
                "            params_image_size=params.IMAGE_SIZE,\n",
                "            # mlflow_uri= training.mlflow_uri\n",
                "        )\n",
                "\n",
                "        return training_config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import urllib.request as request\n",
                "from zipfile import ZipFile\n",
                "import tensorflow as tf\n",
                "import time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "2.16.1\n"
                    ]
                }
            ],
            "source": [
                "# Tensorflow Libraries\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from keras.layers import Dense, Dropout , BatchNormalization\n",
                "from tensorflow.keras.optimizers import Adam\n",
                "from tensorflow.keras import layers,models,Model\n",
                "# from keras.preprocessing.image import ImageDataGenerator\n",
                "# from tensorflow.keras.layers import preprocessing\n",
                "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
                "from tensorflow.keras import mixed_precision\n",
                "mixed_precision.set_global_policy('mixed_float16')\n",
                "\n",
                "\n",
                "print(tf.__version__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "\n",
                "class Training:\n",
                "    def __init__(self, config: TrainingConfig):\n",
                "        self.config = config\n",
                "        \n",
                "    def get_base_model(self):\n",
                "        self.model = tf.keras.models.load_model(\n",
                "            self.config.updated_base_model_path\n",
                "        )\n",
                "\n",
                "    def train_valid_generator(self):\n",
                "        # Define the path to the main data folder\n",
                "        data_dir = os.path.join(self.config.training_data, 'animals')\n",
                "\n",
                "        # Get the list of class names (animal names)\n",
                "        classes = os.listdir(data_dir)\n",
                "\n",
                "        # Split data into train, validation, and test sets for each class\n",
                "        train_data = []\n",
                "        valid_data = []\n",
                "        test_data = []\n",
                "\n",
                "        for class_name in classes:\n",
                "            class_dir = os.path.join(data_dir, class_name)\n",
                "            images = os.listdir(class_dir)\n",
                "            train_images, temp_images = train_test_split(images, train_size=0.70, shuffle=True, random_state=124)\n",
                "            valid_images, test_images = train_test_split(temp_images, train_size=0.70, shuffle=True, random_state=124)\n",
                "\n",
                "            train_data.extend([(os.path.join(class_dir, image), class_name) for image in train_images])\n",
                "            valid_data.extend([(os.path.join(class_dir, image), class_name) for image in valid_images])\n",
                "            test_data.extend([(os.path.join(class_dir, image), class_name) for image in test_images])\n",
                "\n",
                "        # Convert data lists to DataFrames\n",
                "        train_df = pd.DataFrame(train_data, columns=['imgpath', 'labels'])\n",
                "        valid_df = pd.DataFrame(valid_data, columns=['imgpath', 'labels'])\n",
                "        test_df = pd.DataFrame(test_data, columns=['imgpath', 'labels'])\n",
                "\n",
                "        # Reset index for DataFrame consistency\n",
                "        train_df = train_df.reset_index(drop=True)\n",
                "        valid_df = valid_df.reset_index(drop=True)\n",
                "        test_df = test_df.reset_index(drop=True)\n",
                "\n",
                "        # Print information about splits\n",
                "        print(\"----------Train-------------\")\n",
                "        print(train_df.head(5))\n",
                "        print(train_df.shape)\n",
                "        print(\"--------Validation----------\")\n",
                "        print(valid_df.head(5))\n",
                "        print(valid_df.shape)\n",
                "        print(\"----------Test--------------\")\n",
                "        print(test_df.head(5))\n",
                "        print(test_df.shape)\n",
                "\n",
                "        # Set up ImageDataGenerator for image augmentation and preprocessing\n",
                "        generator = ImageDataGenerator(\n",
                "            preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
                "            rotation_range=40 if self.config.params_is_augmentation else 0,\n",
                "            horizontal_flip=True if self.config.params_is_augmentation else False,\n",
                "            width_shift_range=0.2 if self.config.params_is_augmentation else 0,\n",
                "            height_shift_range=0.2 if self.config.params_is_augmentation else 0,\n",
                "            shear_range=0.2 if self.config.params_is_augmentation else 0,\n",
                "            zoom_range=0.2 if self.config.params_is_augmentation else 0,\n",
                "            rescale=1./255,\n",
                "            validation_split=0.20\n",
                "        )\n",
                "\n",
                "        # Set up data generators for training, validation, and testing\n",
                "        self.train_images = generator.flow_from_dataframe(\n",
                "            dataframe=train_df,\n",
                "            x_col='imgpath',\n",
                "            y_col='labels',\n",
                "            target_size=self.config.params_image_size[:-1],\n",
                "            batch_size=self.config.params_batch_size,\n",
                "            class_mode='categorical',\n",
                "            shuffle=True,\n",
                "            subset='training'\n",
                "        )\n",
                "\n",
                "        self.val_images = generator.flow_from_dataframe(\n",
                "            dataframe=valid_df,\n",
                "            x_col='imgpath',\n",
                "            y_col='labels',\n",
                "            target_size=self.config.params_image_size[:-1],\n",
                "            batch_size=self.config.params_batch_size,\n",
                "            class_mode='categorical',\n",
                "            shuffle=False,\n",
                "            subset='validation'\n",
                "        )\n",
                "\n",
                "        self.test_generator = generator.flow_from_dataframe(\n",
                "            dataframe=test_df,\n",
                "            x_col='imgpath',\n",
                "            y_col='labels',\n",
                "            target_size=self.config.params_image_size[:-1],\n",
                "            batch_size=self.config.params_batch_size,\n",
                "            class_mode='categorical',\n",
                "            shuffle=False\n",
                "        )\n",
                "\n",
                "        \n",
                "\n",
                "    \n",
                "    @staticmethod\n",
                "    def save_model(path: Path, model: tf.keras.Model):\n",
                "        model.save(path)\n",
                "\n",
                "\n",
                "\n",
                "    \n",
                "    def train(self):\n",
                "        # self.steps_per_epoch = self.train_generator.samples // (self.train_generator.batch_size*20)\n",
                "        # self.validation_steps = self.valid_generator.samples // (self.valid_generator.batch_size*20)\n",
                "\n",
                "        \n",
                "        self.model.fit(\n",
                "            self.train_images,\n",
                "            steps_per_epoch=len(self.train_images),\n",
                "            validation_data=self.val_images,\n",
                "            validation_steps=len(self.val_images),\n",
                "            epochs=10,\n",
                "            callbacks=[\n",
                "                EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
                "                                    patience = 3,\n",
                "                                    restore_best_weights = True), # if val loss decreases for 10 epochs in a row, stop training,\n",
                "                ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, mode='min') \n",
                "            ]\n",
                "        )\n",
                "        self.model.save_weights('./checkpoints/my_checkpoint')\n",
                "\n",
                "\n",
                "        # self.model.fit(\n",
                "        #     self.train_generator,\n",
                "        #     epochs=self.config.params_epochs,\n",
                "        #     steps_per_epoch=self.steps_per_epoch,\n",
                "        #     validation_steps=self.validation_steps,\n",
                "        #     validation_data=self.valid_generator\n",
                "        # )\n",
                "\n",
                "        self.save_model(\n",
                "            path=self.config.trained_model_path,\n",
                "            model=self.model\n",
                "        )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-04-02 21:13:40,249: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
                        "[2024-04-02 21:13:40,254: INFO: common: yaml file: params.yaml loaded successfully]\n",
                        "[2024-04-02 21:13:40,255: INFO: common: created directory at: artifacts]\n",
                        "[2024-04-02 21:13:40,256: INFO: common: created directory at: artifacts\\training]\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 694 variables whereas the saved optimizer has 4 variables. \n",
                        "  trackable.load_own_variables(weights_store.get(inner_path))\n",
                        "e:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 690 variables whereas the saved optimizer has 2 variables. \n",
                        "  trackable.load_own_variables(weights_store.get(inner_path))\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "----------Train-------------\n",
                        "                                             imgpath    labels\n",
                        "0  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "1  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "2  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "3  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "4  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "(3780, 2)\n",
                        "--------Validation----------\n",
                        "                                             imgpath    labels\n",
                        "0  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "1  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "2  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "3  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "4  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "(1080, 2)\n",
                        "----------Test--------------\n",
                        "                                             imgpath    labels\n",
                        "0  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "1  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "2  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "3  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "4  artifacts\\data_ingestion\\animals\\animals\\antel...  antelope\n",
                        "(540, 2)\n",
                        "Found 3024 validated image filenames belonging to 90 classes.\n",
                        "Found 216 validated image filenames belonging to 90 classes.\n",
                        "Found 540 validated image filenames belonging to 90 classes.\n",
                        "Epoch 1/10\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
                        "  self._warn_if_super_not_called()\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m 17/202\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08:39\u001b[0m 42s/step - accuracy: 0.0041 - loss: 5.2824"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    config = ConfigurationManager()\n",
                "    training_config = config.get_training_config()\n",
                "    training = Training(config=training_config)\n",
                "    training.get_base_model()\n",
                "    training.train_valid_generator()\n",
                "    training.train()\n",
                "    \n",
                "except Exception as e:\n",
                "    raise e\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Training:\n",
                "    def __init__(self, config: TrainingConfig):\n",
                "        self.config = config\n",
                "\n",
                "    \n",
                "    def get_base_model(self):\n",
                "        self.model = tf.keras.models.load_model(\n",
                "            self.config.updated_base_model_path\n",
                "        )\n",
                "\n",
                "    def train_valid_generator(self):\n",
                "\n",
                "        datagenerator_kwargs = dict(\n",
                "            rescale = 1./255,\n",
                "            validation_split=0.20\n",
                "        )\n",
                "\n",
                "        dataflow_kwargs = dict(\n",
                "            target_size=self.config.params_image_size[:-1],\n",
                "            batch_size=self.config.params_batch_size,\n",
                "            interpolation=\"bilinear\"\n",
                "        )\n",
                "\n",
                "        valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
                "            **datagenerator_kwargs\n",
                "        )\n",
                "\n",
                "        self.valid_generator = valid_datagenerator.flow_from_directory(\n",
                "            directory=self.config.training_data,\n",
                "            subset=\"validation\",\n",
                "            shuffle=False,\n",
                "            **dataflow_kwargs\n",
                "        )\n",
                "\n",
                "        if self.config.params_is_augmentation:\n",
                "            train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
                "                rotation_range=40,\n",
                "                horizontal_flip=True,\n",
                "                width_shift_range=0.2,\n",
                "                height_shift_range=0.2,\n",
                "                shear_range=0.2,\n",
                "                zoom_range=0.2,\n",
                "                **datagenerator_kwargs\n",
                "            )\n",
                "        else:\n",
                "            train_datagenerator = valid_datagenerator\n",
                "\n",
                "        self.train_generator = train_datagenerator.flow_from_directory(\n",
                "            directory=self.config.training_data,\n",
                "            subset=\"training\",\n",
                "            shuffle=True,\n",
                "            **dataflow_kwargs\n",
                "        )\n",
                "\n",
                "    \n",
                "    @staticmethod\n",
                "    def save_model(path: Path, model: tf.keras.Model):\n",
                "        model.save(path)\n",
                "\n",
                "\n",
                "\n",
                "    \n",
                "    def train(self):\n",
                "        self.steps_per_epoch = self.train_generator.samples // (self.train_generator.batch_size*20)\n",
                "        self.validation_steps = self.valid_generator.samples // (self.valid_generator.batch_size*20)\n",
                "\n",
                "        self.model.fit(\n",
                "            self.train_generator,\n",
                "            epochs=self.config.params_epochs,\n",
                "            steps_per_epoch=self.steps_per_epoch,\n",
                "            validation_steps=self.validation_steps,\n",
                "            validation_data=self.valid_generator\n",
                "        )\n",
                "\n",
                "        self.save_model(\n",
                "            path=self.config.trained_model_path,\n",
                "            model=self.model\n",
                "        )\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-04-02 20:08:12,631: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
                        "[2024-04-02 20:08:12,636: INFO: common: yaml file: params.yaml loaded successfully]\n",
                        "[2024-04-02 20:08:12,638: INFO: common: created directory at: artifacts]\n",
                        "[2024-04-02 20:08:12,640: INFO: common: created directory at: artifacts\\training]\n",
                        "Found 1080 images belonging to 90 classes.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 690 variables whereas the saved optimizer has 4 variables. \n",
                        "  trackable.load_own_variables(weights_store.get(inner_path))\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Found 4320 images belonging to 90 classes.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "e:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
                        "  self._warn_if_super_not_called()\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[1m 5/14\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6:49\u001b[0m 45s/step - accuracy: 0.0000e+00 - loss: 5.2533"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mget_base_model()\n\u001b[0;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain_valid_generator()\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
                        "Cell \u001b[1;32mIn[8], line 67\u001b[0m, in \u001b[0;36mTraining.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_generator\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_generator\u001b[38;5;241m.\u001b[39msamples \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_generator\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid_generator\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(\n\u001b[0;32m     76\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtrained_model_path,\n\u001b[0;32m     77\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m     78\u001b[0m )\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:325\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    324\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 325\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    327\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    328\u001b[0m     )\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
                        "File \u001b[1;32me:\\projects\\anidex\\anidex-y\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    config = ConfigurationManager()\n",
                "    training_config = config.get_training_config()\n",
                "    training = Training(config=training_config)\n",
                "    training.get_base_model()\n",
                "    training.train_valid_generator()\n",
                "    training.train()\n",
                "    \n",
                "except Exception as e:\n",
                "    raise e\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ModelTrainer:\n",
                "    def __init__(self, config: TrainingConfig):\n",
                "        self.config = config\n",
                "        \n",
                "\n",
                "\n",
                "    def initiate_model_trainer(self):\n",
                "        DF = pd.read_csv(self.config.training_data)\n",
                "\n",
                "\n",
                "        imp_feature = ['propertyType',\n",
                "                       'locality',\n",
                "                       'furnishing',\n",
                "                       'city',\n",
                "                       'bedrooms',\n",
                "                       'bathrooms',\n",
                "                       'RentOrSale',]\n",
                "\n",
                "        DF = remove_outliers_iqr(DF)\n",
                "        try:\n",
                "            logging.info(\"Split training and test input data\")\n",
                "            X_train, X_test, y_train, y_test = train_test_split(\n",
                "                DF[imp_feature], DF[[\"exactPrice\"]], test_size=0.33, random_state=42)\n",
                "            \n",
                "            print(y_train)\n",
                "            # To convert y_train to required format\n",
                "            y_train = np.ravel(y_train)\n",
                "            print(y_train)\n",
                "\n",
                "            models = {\n",
                "                \"RandomForest\": RandomForestRegressor(),\n",
                "                \"DecisionTree\": DecisionTreeRegressor(),\n",
                "                \"GradientBoosting\": GradientBoostingRegressor(),\n",
                "                \"LinearRegression\": LinearRegression(),\n",
                "                \"XGBRegressor\": XGBRegressor(),\n",
                "                \"CatBoostRegressor\": CatBoostRegressor(verbose=False),\n",
                "\n",
                "\n",
                "                \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
                "            }\n",
                "            print(\"OKK 1\")\n",
                "\n",
                "            params = self.config.model_params\n",
                "            print(params.MODELS)\n",
                "            print(\"OKK 2\")\n",
                "            models = {key: value for key, value in models.items() if key in params.MODELS}\n",
                "            print(models)\n",
                "\n",
                "            print(\"OKK 3\")\n",
                "\n",
                "\n",
                "            model_report,parameters = evaluate_models(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
                "                                                 models=models, param=params.MODELS)\n",
                "            print(\"OKK 4\")\n",
                "            logging.info(f\"models -> {model_report}\")\n",
                "\n",
                "\n",
                "            \n",
                "        \n",
                "        \n",
                "            # To get best model score from dict\n",
                "            best_model_score = max(sorted(model_report.values()))\n",
                "\n",
                "            # To get best model name from dict\n",
                "\n",
                "            best_model_name = list(model_report.keys())[\n",
                "                list(model_report.values()).index(best_model_score)\n",
                "            ]\n",
                "            best_model = models[best_model_name]\n",
                "\n",
                "            if best_model_score < 0.6:\n",
                "                raise CustomException(\"No best model found\")\n",
                "            logging.info(\n",
                "                f\"Best found model on both training and testing dataset\")\n",
                "\n",
                "            save_object(\n",
                "                file_path=self.config.trained_model_file_path,\n",
                "                obj=best_model\n",
                "            )\n",
                "\n",
                "            predicted = best_model.predict(X_test)\n",
                "\n",
                "            r2_square = r2_score(y_test, predicted)\n",
                "            mae = mean_absolute_error(y_test, predicted)\n",
                "            print(mae)\n",
                "\n",
                "            # Log into MLflow\n",
                "            # self.log_model_into_mlflow(model_report,parameters,y_test,predicted,\"Full Data Model\")\n",
                "\n",
                "            return model_report\n",
                "\n",
                "        except Exception as e:\n",
                "            raise CustomException(e, sys)\n",
                "\n",
                "    def initiate_model_trainer_rent(self):\n",
                "        DF = pd.read_csv(self.config.training_data)\n",
                "\n",
                "\n",
                "        imp_feature = ['propertyType',\n",
                "                       'locality',\n",
                "                       'furnishing',\n",
                "                       'city',\n",
                "                       'bedrooms',\n",
                "                       'bathrooms',\n",
                "                       'RentOrSale',]\n",
                "        \n",
                "        DF = DF[DF[\"RentOrSale\"] == 1]\n",
                "\n",
                "\n",
                "        DF = remove_outliers_iqr(DF)\n",
                "        try:\n",
                "            logging.info(\"Split training and test input data\")\n",
                "            X_train, X_test, y_train, y_test = train_test_split(\n",
                "                DF[imp_feature], DF[[\"exactPrice\"]], test_size=0.33, random_state=42)\n",
                "            \n",
                "            print(y_train)\n",
                "            # To convert y_train to required format\n",
                "            y_train = np.ravel(y_train)\n",
                "            print(y_train)\n",
                "\n",
                "            models = {\n",
                "                \"RandomForest\": RandomForestRegressor(),\n",
                "                \"DecisionTree\": DecisionTreeRegressor(),\n",
                "                \"GradientBoosting\": GradientBoostingRegressor(),\n",
                "                \"LinearRegression\": LinearRegression(),\n",
                "                \"XGBRegressor\": XGBRegressor(),\n",
                "                \"CatBoostRegressor\": CatBoostRegressor(),\n",
                "\n",
                "                \"AdaBoostRegressor\": AdaBoostRegressor(),\n",
                "            }\n",
                "            print(\"OKK 1\")\n",
                "\n",
                "            params = self.config.model_params\n",
                "            print(params.MODELS)\n",
                "            print(\"OKK 2\")\n",
                "            models = {key: value for key, value in models.items() if key in params.MODELS}\n",
                "            print(models)\n",
                "\n",
                "            print(\"OKK 3\")\n",
                "\n",
                "\n",
                "            model_report,parameters = evaluate_models(X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test,\n",
                "                                                 models=models, param=params.MODELS)\n",
                "            print(\"OKK 4\")\n",
                "            logging.info(f\"models -> {model_report}\")\n",
                "\n",
                "\n",
                "            \n",
                "        \n",
                "        \n",
                "            # To get best model score from dict\n",
                "            best_model_score = max(sorted(model_report.values()))\n",
                "\n",
                "            # To get best model name from dict\n",
                "\n",
                "            best_model_name = list(model_report.keys())[\n",
                "                list(model_report.values()).index(best_model_score)\n",
                "            ]\n",
                "            best_model = models[best_model_name]\n",
                "\n",
                "            if best_model_score < 0.3:\n",
                "                raise CustomException(\"No best model found\")\n",
                "            logging.info(\n",
                "                f\"Best found model on both training and testing dataset\")\n",
                "\n",
                "            save_object(\n",
                "                file_path=self.config.trained_model_file_path_rent,\n",
                "                obj=best_model\n",
                "            )\n",
                "\n",
                "            predicted = best_model.predict(X_test)\n",
                "\n",
                "            r2_square = r2_score(y_test, predicted)\n",
                "            mae = mean_absolute_error(y_test, predicted)\n",
                "            print(mae)\n",
                "\n",
                "\n",
                "            # self.log_model_into_mlflow(model_report,parameters,y_test,predicted,\"Rent Data Model\")\n",
                "\n",
                "            return model_report\n",
                "\n",
                "        except Exception as e:\n",
                "            raise CustomException(e, sys)\n",
                "\n",
                "    \n",
                "    \n",
                "    def log_model_into_mlflow(self, model_report,parameters,y_test,predicted,run_name):\n",
                "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
                "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
                "        \n",
                "        with mlflow.start_run(run_name=run_name):\n",
                "            for model_name, model in model_report.items():\n",
                "                mlflow.log_params(parameters)  # Log model parameters\n",
                "                mlflow.log_metric(\"mean_squared_error\", mean_squared_error(y_test, predicted))  # Log MSE\n",
                "                mlflow.log_metric(\"mean_absolute_error\", mean_absolute_error(y_test, predicted))  # Log MSE\n",
                "                mlflow.log_metric(\"r2_score\", r2_score(y_test, predicted))  # Log R2 Score\n",
                "                \n",
                "                # Model registry does not work with file store\n",
                "                if tracking_url_type_store != \"file\":\n",
                "                    # Register the model\n",
                "                    # There are other ways to use the Model Registry, which depends on the use case,\n",
                "                    # please refer to the doc for more information:\n",
                "                    # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
                "                    mlflow.sklearn.log_model(model, f\"{model_name}_model\", registered_model_name=model_name)\n",
                "                else:\n",
                "                    mlflow.sklearn.log_model(model, f\"{model_name}_model\")\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-18 13:11:54,732: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
                        "[2024-03-18 13:11:54,805: INFO: common: yaml file: params.yaml loaded successfully]\n",
                        "[2024-03-18 13:11:54,811: INFO: common: created directory at: artifacts]\n",
                        "[2024-03-18 13:11:54,815: INFO: common: created directory at: artifacts/training]\n",
                        "[2024-03-18 13:11:54,921: INFO: 614572267: Split training and test input data]\n",
                        "       exactPrice\n",
                        "9614      12000.0\n",
                        "1347      24000.0\n",
                        "22355   8700000.0\n",
                        "8487      15000.0\n",
                        "15059     15000.0\n",
                        "...           ...\n",
                        "13021     25000.0\n",
                        "13955      6500.0\n",
                        "6089       6500.0\n",
                        "991        8000.0\n",
                        "18495   2580000.0\n",
                        "\n",
                        "[14051 rows x 1 columns]\n",
                        "[1.20e+04 2.40e+04 8.70e+06 ... 6.50e+03 8.00e+03 2.58e+06]\n",
                        "OKK 1\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}}\n",
                        "OKK 2\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B248807F0>}\n",
                        "OKK 3\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B248807F0>}\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}} utils\n",
                        "[2024-03-18 13:12:29,041: INFO: common: utils model trainer : {'DecisionTree': 0.8360509744841325, 'CatBoostRegressor': 0.88965291486411} , {'loss_function': 'RMSE', 'verbose': False}]\n",
                        "OKK 4\n",
                        "[2024-03-18 13:12:29,047: INFO: 614572267: models -> {'DecisionTree': 0.8360509744841325, 'CatBoostRegressor': 0.88965291486411}]\n",
                        "[2024-03-18 13:12:29,049: INFO: 614572267: Best found model on both training and testing dataset]\n",
                        "459164.9285455783\n",
                        "[2024-03-18 13:12:29,161: INFO: 614572267: Split training and test input data]\n",
                        "       exactPrice\n",
                        "1064      40000.0\n",
                        "380       30000.0\n",
                        "2511      35000.0\n",
                        "12997      8000.0\n",
                        "11548     20000.0\n",
                        "...           ...\n",
                        "16843     27000.0\n",
                        "6166      10000.0\n",
                        "6384       4500.0\n",
                        "1093      20000.0\n",
                        "8533      15000.0\n",
                        "\n",
                        "[8894 rows x 1 columns]\n",
                        "[40000. 30000. 35000. ...  4500. 20000. 15000.]\n",
                        "OKK 1\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}}\n",
                        "OKK 2\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24254340>}\n",
                        "OKK 3\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24254340>}\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}} utils\n",
                        "[2024-03-18 13:12:58,726: INFO: common: utils model trainer : {'DecisionTree': 0.4290943803412891, 'CatBoostRegressor': 0.6850730978187893} , {'loss_function': 'RMSE', 'verbose': False}]\n",
                        "OKK 4\n",
                        "[2024-03-18 13:12:58,734: INFO: 614572267: models -> {'DecisionTree': 0.4290943803412891, 'CatBoostRegressor': 0.6850730978187893}]\n",
                        "[2024-03-18 13:12:58,736: INFO: 614572267: Best found model on both training and testing dataset]\n",
                        "3743.400661147413\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    config = ConfigurationManager()\n",
                "    model_training_config = config.get_model_training_config()\n",
                "    model_training = ModelTrainer(config=model_training_config)\n",
                "    model_training.initiate_model_trainer()\n",
                "    model_training.initiate_model_trainer_rent()\n",
                "except Exception as e:\n",
                "    raise e\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['MLFLOW_TRACKING_URI']='https://dagshub.com/Rajarshi12321/My-Sweet-Home.mlflow'\n",
                "os.environ['MLFLOW_TRACKING_USERNAME']='Rajarshi12321'\n",
                "os.environ['MLFLOW_TRACKING_PASSWORD']='ba0cfe97e529787e678d28321906247cfce4fb43'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-18 13:13:45,373: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
                        "[2024-03-18 13:13:45,390: INFO: common: yaml file: params.yaml loaded successfully]\n",
                        "[2024-03-18 13:13:45,398: INFO: common: created directory at: artifacts]\n",
                        "[2024-03-18 13:13:45,402: INFO: common: created directory at: artifacts/training]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[2024-03-18 13:13:45,500: INFO: 614572267: Split training and test input data]\n",
                        "       exactPrice\n",
                        "9614      12000.0\n",
                        "1347      24000.0\n",
                        "22355   8700000.0\n",
                        "8487      15000.0\n",
                        "15059     15000.0\n",
                        "...           ...\n",
                        "13021     25000.0\n",
                        "13955      6500.0\n",
                        "6089       6500.0\n",
                        "991        8000.0\n",
                        "18495   2580000.0\n",
                        "\n",
                        "[14051 rows x 1 columns]\n",
                        "[1.20e+04 2.40e+04 8.70e+06 ... 6.50e+03 8.00e+03 2.58e+06]\n",
                        "OKK 1\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}}\n",
                        "OKK 2\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24254190>}\n",
                        "OKK 3\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24254190>}\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}} utils\n",
                        "[2024-03-18 13:14:17,746: INFO: common: utils model trainer : {'DecisionTree': 0.8390018829702992, 'CatBoostRegressor': 0.88965291486411} , {'loss_function': 'RMSE', 'verbose': False}]\n",
                        "OKK 4\n",
                        "[2024-03-18 13:14:17,752: INFO: 614572267: models -> {'DecisionTree': 0.8390018829702992, 'CatBoostRegressor': 0.88965291486411}]\n",
                        "[2024-03-18 13:14:17,756: INFO: 614572267: Best found model on both training and testing dataset]\n",
                        "459164.9285455783\n",
                        "[2024-03-18 13:14:17,881: INFO: 614572267: Split training and test input data]\n",
                        "       exactPrice\n",
                        "1064      40000.0\n",
                        "380       30000.0\n",
                        "2511      35000.0\n",
                        "12997      8000.0\n",
                        "11548     20000.0\n",
                        "...           ...\n",
                        "16843     27000.0\n",
                        "6166      10000.0\n",
                        "6384       4500.0\n",
                        "1093      20000.0\n",
                        "8533      15000.0\n",
                        "\n",
                        "[8894 rows x 1 columns]\n",
                        "[40000. 30000. 35000. ...  4500. 20000. 15000.]\n",
                        "OKK 1\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}}\n",
                        "OKK 2\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24880520>}\n",
                        "OKK 3\n",
                        "{'DecisionTree': DecisionTreeRegressor(), 'CatBoostRegressor': <catboost.core.CatBoostRegressor object at 0x0000021B24880520>}\n",
                        "{'DecisionTree': {'criterion': ['squared_error', 'friedman_mse'], 'splitter': ['best', 'random']}, 'CatBoostRegressor': {'verbose': [False]}} utils\n",
                        "[2024-03-18 13:14:42,713: INFO: common: utils model trainer : {'DecisionTree': 0.44207845729699424, 'CatBoostRegressor': 0.6850730978187893} , {'loss_function': 'RMSE', 'verbose': False}]\n",
                        "OKK 4\n",
                        "[2024-03-18 13:14:42,713: INFO: 614572267: models -> {'DecisionTree': 0.44207845729699424, 'CatBoostRegressor': 0.6850730978187893}]\n",
                        "[2024-03-18 13:14:42,720: INFO: 614572267: Best found model on both training and testing dataset]\n",
                        "3743.400661147413\n"
                    ]
                }
            ],
            "source": [
                "try:\n",
                "    config = ConfigurationManager()\n",
                "    model_training_config = config.get_model_training_config()\n",
                "    model_training = ModelTrainer(config=model_training_config)\n",
                "    model_training.initiate_model_trainer()\n",
                "    model_training.initiate_model_trainer_rent()\n",
                "except Exception as e:\n",
                "    raise e\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "housing",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
